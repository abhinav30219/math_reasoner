{
  "training": {
    "num_episodes": 1000,
    "batch_size": 16,
    "gradient_accumulation_steps": 4,
    "learning_rate": 5e-6,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,
    "log_interval": 10,
    "save_interval": 100,
    "seed": 42
  },
  "grpo": {
    "kl_coef": 0.05,
    "clip_range": 0.2,
    "group_size": 8,
    "mini_batch_size": 4,
    "num_epochs": 4,
    "temperature": 0.7,
    "top_p": 0.9
  },
  "model": {
    "model_type": "qwen",
    "model_name_or_path": "Qwen/Qwen2.5-7B",
    "max_length": 2048,
    "precision": "bf16",
    "use_flash_attention": true,
    "use_lora": true,
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.05,
      "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
      "bias": "none"
    }
  },
  "dataset": {
    "dataset_paths": "numina/numina-math-filtered",
    "probabilities": null,
    "max_samples": 500000,
    "problem_prefix": "Problem: ",
    "solution_prefix": "Solution: ",
    "train_split": "train",
    "eval_split": "test",
    "eval_ratio": 0.03
  },
  "output": {
    "output_dir": "models/grpo-qwen",
    "logging_dir": "logs/grpo",
    "use_wandb": false,
    "wandb_project": "ai-math-reasoning",
    "wandb_run_name": "grpo-run"
  },
  "distributed": {
    "use_deepspeed": false,
    "deepspeed_config": "configs/deepspeed/ds_config_zero2.json",
    "use_accelerate": false,
    "gradient_checkpoint": true,
    "ddp_find_unused_parameters": false
  }
}
